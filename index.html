<!DOCTYPE html>
<html data-wf-site="554481fae11f375d20614d0e" data-wf-page="554481fae11f375d20614d10" data-wf-status="1" class="w-mod-js w-mod-no-touch w-mod-video w-mod-no-ios wf-lato-n1-active wf-lato-i1-active wf-lato-n3-active wf-lato-i3-active wf-lato-n4-active wf-lato-i4-active wf-lato-n7-active wf-lato-i7-active wf-lato-n9-active wf-lato-i9-active wf-roboto-n3-active wf-roboto-n4-active wf-roboto-n5-active wf-robotocondensed-n3-active wf-robotocondensed-n4-active wf-robotocondensed-n7-active wf-robotoslab-n3-active wf-robotoslab-n4-active wf-robotoslab-n7-active wf-arbutusslab-n4-active wf-active">
	
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>zhouyi</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="generator" content="Webflow"><link rel="stylesheet" type="text/css" href="./zhouyi_files/zhouyi.webflow.9320a162c.css"><script src="./zhouyi_files/webfont.js"></script><link rel="stylesheet" href="./zhouyi_files/css"><script>WebFont.load({
  google: {
    families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Roboto:300,regular,500","Roboto Condensed:300,regular,700","Roboto Slab:300,regular,700","Arbutus Slab:regular"]
  }
});</script><script type="text/javascript" src="./zhouyi_files/modernizr-2.7.1.js"></script><link rel="shortcut icon" type="image/x-icon" href="https://daks2k3a4ib2z.cloudfront.net/537a7966446efcce2d6dcce5/537d2eeceffa99671899c195_New%20icon.ico"><link rel="apple-touch-icon" href="https://daks2k3a4ib2z.cloudfront.net/537a7966446efcce2d6dcce5/537d2ecdeffa99671899c192_metric-webclip.png">
<style>
	/* Create two unequal columns that floats next to each other */
.column {
  float: left;
  padding: 0px;
  height: 200px; /* Should be removed. Only for demonstration */
}

.left {
  width: 40%;
  border-radius: 0px;
  border: 0;
  display: inline-block;
  position: relative;
  vertical-align: center;
}

.right {
  width: 60%;
  border-radius: 0px;
  border: 0;
  display: inline-block;
  position: relative;
  vertical-align: center;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
  vertical-align: center;
}

.image {
    border-radius: 5px;
    border: 0;
    display: inline-block;
    position: relative;
	
}


</style>
</head>
<body>
<div data-collapse="medium" data-animation="default" data-duration="400" data-contain="1" class="w-nav navigation">
	<div class="w-container">
		<nav role="navigation" class="w-nav-menu nav-menu">
			<a href="./index.html#top" class="w-nav-link nav-link w--current" style="max-width: 940px;">About</a>
			<a href="./index.html#research" class="w-nav-link nav-link" style="max-width: 940px;">Research</a>
			<a href="./index.html#music" class="w-nav-link nav-link" style="max-width: 940px;">Music</a>
		</nav>
	<div class="w-nav-button nav-link menu"><div class="w-icon-nav-menu"></div></div>
	</div>
	<div class="w-nav-overlay" data-wf-ignore=""></div>
</div>

<div id="top" class="w-section section main">
	<div class="w-container selintro-container" >
		<div class="w-row">
			<div class="w-col w-col-5">
				<img width="306" src="./zhouyi_files/zy_laopera.png">
			</div>
			<div class="w-col w-col-7">
					<div class="w-col w-col-8">
						<div><h1><font color=white> Yi Zhou</font></h1></div>
						<div class="email-text"><font color=white>yizho@adobe.com</font>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="cv_zhouyi.pdf" target="_blank"><font color=white>CV</font></a></div>
					</div>
					
					
					<div><p style="color:white"><br><br><br><br>I am a research scientist at Adobe. I received my PhD from the University of Southern California under the supervision of <a href="https://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Dr. Hao Li</a> and my Master's and Bachelor's degrees from Shanghai Jiao Tong University under the supervision of Dr. Shuangjiu Xiao. My research focuses on 3D learning and 3D vision, especially on creating digital humans and autonomous virtual avatars. My works mainly lie in 3D human modeling, reconstruction, motion synthesis and simulation.</p>
						
					</div>

			</div>
			
		</div>
	</div>
</div>

<div id="news" class="w-section section press" >

	<div class="project-contatiner w-container"><h1 class="main-heading">News</h1>

	<p style="color:black"> - Debuting my first original song “痛苦都来自幻想”.</p>

	<p style="color:black"> - Digital Salon was presented at Siggraph Asia Real-Time Live 2024.</p>
	
	<p style="color:black"> - I will serve in the Siggraph Asia 2024 Technical Papers Committee.</p>

	<p style="color:black"> - I am co-organizing the CVPR 2024 workshop <a href="https://syntagen.github.io/.">SyntaGen - Harnessing Generative Models for Synthetic Visual Datasets</a>. </p>
	
	<p style="color:black"> - The internship positions with me for 2024 have been filled. But I am always eager to connect with passionate individuals interested in future opportunities. Internship selections are typically made in late January and early February, but occasions may arise for mid-year openings. If you're interested in collaborating or exploring possibilities at any point in the future, I encourage you to email me directly. Your initiative and interest are highly valued, and I welcome your contact at any time.</p>

	<p style="color:black"> - I presented Project Poseable at <a href="https://www.adobe.com/max.html">Adobe Max Sneaks 2023</a> . Watch the <a href="https://www.youtube.com/live/barsu1NWE4s?si=g9Yxspi-Ub2Q-1XG&t=2479.">live demo</a>. </p>

	<p style="color:black"> - I served in the Siggraph 2023 Technical Papers Committee. I will chair the session "Motion Recipes and Simulation" on Monday, 7 August 10:45am - 12:15pm PDT.</p>

</div>



<div id="research" class="w-section section press" >

	
	<div class="project-contatiner w-container"><h1 class="main-heading">Research</h1>

	
	<hr class="myhr" ></hr>




	<div class="project-contatiner w-container">
		<h1 class="project-heading" >3D Learning</h1>
		<p>This part of work is about designing the general architecture and understanding the mechanism of deep learning for 3D data. </p>
		
		<br>
		<b>Publications:</b>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<video autoplay muted playsinline loop src="zhouyi_files/dmesh2.mp4" width="90%" height="130%"></video>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>DMesh++: An Efficient Differentiable Mesh for Complex Shapes</b>

						<br>Sanghyun Son (Primary Intern), Matheus Gadelha, Yang Zhou, Matthew Fisher, Zexiang Xu, Yi-Ling Qiao, Ming C. Lin, <b>Yi Zhou</b>
						<br><em>2024</em> [<a href="https://arxiv.org/abs/2412.16776">paper</a>] [<a href="https://sonsang.github.io/dmesh2-project/">project page</a>] 
				</div>
			</div>

			<div>
				<br>
				<br>
			</div>


			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<video autoplay muted playsinline loop src="zhouyi_files/bunny_teaser_no_step.mp4" height="70%"></video>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>DMesh: A Differentiable Representation for General Meshes</b>

						<br>Sanghyun Son (Primary Intern), Matheus Gadelha, Yang Zhou, Zexiang Xu, Ming C. Lin, <b>Yi Zhou</b>
						<br><em>NeurIPS 2024</em> [<a href="https://www.cs.umd.edu/~shh1295/dmesh/full.pdf">paper</a>] [<a href="https://sonsang.github.io/dmesh-project/">project page</a>] [<a href="https://github.com/SonSang/dmesh">code</a>]   
				</div>
			</div>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img width="90%" height="80%" src="zhouyi_files/lrmzero.png" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>LRM-Zero: Training Large Reconstruction Models with Synthesized Data</b>

						<br>Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, <b>Yi Zhou</b>, Sören Pirk, Arie Kaufman, Xin Sun, Hao Tan
						<br><em>NeurIPS 2024</em> [<a href="https://arxiv.org/abs/2406.09371">paper</a>] [<a href="https://desaixie.github.io/lrm-zero/">project page</a>] [<a href="https://github.com/desaixie/zeroverse">code</a>]   
				</div>
			</div>


			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img width="90%" height="80%" src="zhouyi_files/carve3d.png" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Carve3d: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning</b>

						<br>Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, <b>Yi Zhou</b>, Sai Bi, Sören Pirk, Arie E Kaufman
						<br><em>CVPR 2024</em> [<a href="https://arxiv.org/abs/2312.13980">paper</a>] [<a href="https://desaixie.github.io/carve-3d/">project page</a>] [<a href="https://github.com/desaixie/carve3d">code</a>]   
				</div>
			</div>
			

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img width="90%" height="60%" src="zhouyi_files/meshcnn.jpg" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Fully Convolutional Mesh Autoencoder Using Efficient Spacially Varying Kernals</b>
						<br><b>Yi Zhou</b>, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, Yaser Sheikh
						<br><em>Neurips 2020</em> [<a href="https://arxiv.org/pdf/2006.04325.pdf">paper</a>]  [<a href="project_vcmeshcnn/vcmeshcnn.html">project page</a>] 
				</div>
		 	</div>

		  	<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img width="90%" height="40%" src="zhouyi_files/rotation.jpg" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>On The Continguity of Rotation Representations in Neural Networks</b>
						<br><b>Yi Zhou*</b>, Connelly Barnes*, Jingwan Lu, Jimei Yang, Hao Li
						<br><em>CVPR 2019</em> [<a href="https://arxiv.org/pdf/1812.07035.pdf">paper</a>]  [<a href="project_rotation/rotation.html">project page</a>] 
				</div>
		 	 </div>

		  

	</div>

	<h1 class="project-heading" >3D Hair</h1>
		<p>This part of work is about hair modeling and simulation. </p>
		<br>
		<b>Publications:</b>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./project_hair/digital_salon_teaser.jpg" width="90%"/>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation.</b>
						<br>Chengan He (primary intern)*, Jorge Alejandro Amador Herrera (primary intern)*, <b>Yi Zhou*</b>, Zhixin Shu, Xin Sun, Yao Feng, Sören Pirk, Dominik L. Michels, Meng Zhang, Tuanfeng Y. Wang, Holly Rushmeier
						<br><em>Siggraph Asia Real-Time Live 2024</em> [<a href="https://digital-salon.github.io/">project page</a>] [<a href="https://youtu.be/PsbuACCnQFI?t=1244">RTL recording</a>] 
						<p>&nbsp;</p> 
				</div>
			</div>

			<div>
				<br>
				<br>
			</div>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					
					<video width="90%" height="140%" loop autoplay="autoplay" muted source src="zhouyi_files/ams.mp4" type="video/mp4" controls></video>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Augmented Mass-Spring model for Real-Time Dense Hair Simulation</b>
						<br>Jorge Alejandro Amador Herrera (primary intern), <b>Yi Zhou</b>, Xin Sun, Zhixin Shu, Chengan He, Soren Pirk, Dominik L. Michels
						<br><em>2024</em> [<a href="https://agrosamad.github.io/AMS/">project page</a>]	[<a href="https://arxiv.org/abs/2412.17144">arXiv</a>] 
						<p>&nbsp;</p> 
				</div>
			</div>

			<p><br></p>
			<p><br></p>
			<p><br></p>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./project_hair/hair20k_teaser.jpg" width="90%"/>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Hair20K: A Large 3D Hairstyle Database.</b>
						<br><b>Yi Zhou</b>, Xin Sun, Chengan He
						<br>2024[<a href="project_hair/hair20k.html">project page</a>]
						<p>&nbsp;</p> 
				</div>
			</div>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./project_hair/perm_teaser.png" width="90%"/>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Perm: A Parametric Representation for Multi-Style 3D Hair Modeling.</b>
						<br>Chengan He (primary intern), Xin Sun, Zhixin Shu, Fujun Luan, Soren Pirk, Jorge Alejandro Amador Herrera, Dominik L. Michels, Tuanfeng Y. Wang, Meng Zhang, Holly Rushmeier and <b>Yi Zhou</b>
						<br><em>2024 ICLR (Spotlight)</em> [<a href="https://arxiv.org/abs/2407.19451">arXiv</a>]  [<a href="https://cs.yale.edu/homes/che/projects/perm/">project page</a>] [<a href="https://github.com/c-he/perm">code</a>] [<a href="project_hair/hair20k.html">data</a>]
						<p>&nbsp;</p> 
				</div>
			</div>

			
		
			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./zhouyi_files/hair.jpg" width="90%"/>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>HairNet: Single-View Hair Reconstruction Using Convolutional Neural Networks</b>
						<br><b>Yi Zhou</b>, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung, Xin Tong, Hao Li
						<br><em>ECCV 2018</em> [<a href="https://arxiv.org/pdf/1806.07467.pdf">paper</a>] [<a href="https://youtu.be/MLnS-gTWc9w">video</a>] [<a href="https://github.com/papagina/HairNet_DataSetGeneration">dataset for 40 thousand 3D hair models</a>]
						<p>&nbsp;</p> 
				</div>
			</div>

		  

	</div>
	
	<hr class="myhr" ></hr>
	
	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Motion</h1>
		<p>This part of work is about generating vivid motion for 3D Avatars. Here we investigate in topics on motion prior, motion editing, long-term inbetweening, motion style editing, hand-object interaction, human-scene interaction and secondary motion simulation. </p>

		<br>
		
		<b>Publications:</b>

		

		<div class="row">

			<div class="column left" style="background-color:FFFF;">
				<img src="zhouyi_files/mgd_grasp.png" width="85%">
			</div>
			<div class="column right" style="background-color:ffff;">
				
				<p>
					<b>Multi-Modal Hand-Object Interaction Generation.</b>
					<br>Jinkun Cao (Primary Intern), Jingyuan Liu, Kris Kitani,<b>Yi Zhou</b>
					<br><em>2024</em> [<a href="[arxiv]">paper</a>] 

			</div>
		  </div>
		  <div class="row">

			<div class="column left" style="background-color:FFFF;">
				<img src="zhouyi_files/grip.png" height="100%">
			</div>
			<div class="column right" style="background-color:ffff;">
				
				<p>
					<b>GRIP: Generating Interaction Poses Conditioned on Object and Body Motion.</b>
					<br>Omid Taheri (Primary Intern), <b>Yi Zhou</b>, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, Michael J. Black
					<br><em>3DV 2024</em> [<a href="https://arxiv.org/abs/2308.11617">paper</a>]  [<a href="https://otaheri.github.io/publication/2023_grip/">project page</a>] 

			</div>
		  </div>


		<div class="row">
			<div class="column left" style="background-color:FFFF;">
				<video autoplay muted playsinline loop src="zhouyi_files/face-tracking.mp4" height="90%">
			</div>
			<div class="column right" style="background-color:ffff;">
				
				<p>
					<b>Fast Complementary Dynamics via Skinning Eigenmodes.</b>
					<br>Otman Benchekroun, Jiayi Eris Zhang, Siddhartha Chaudhuri, Eitan Grinspun1, <b>Yi Zhou</b>, Alec Jacobson
					<br><em>Siggraph 2023</em> [<a href="https://www.dgp.toronto.edu/projects/fast_complementary_dynamics_site/Fast_Complementary_Dynamics_Low_res.pdf">paper</a>]  [<a href="https://www.dgp.toronto.edu/projects/fast_complementary_dynamics_site/">project page</a>] 

			</div>
		  </div>

		<div class="row">
			<div class="column left" style="background-color:FFFF;">
				<img src="zhouyi_files/nemf.gif" height="90%">
			</div>
			<div class="column right" style="background-color:ffff;">
				
				<p>
					<b>NeMF:Neural Motion Fields for Kinematic Animation.</b>
					<br>Chengan He (Primary Intern), Jun Saito, James Zachary, Holly Rushmeier, <b>Yi Zhou</b>
					<br><em>Neurips 2022 (Spotlight Paper)</em> [<a href="https://arxiv.org/pdf/2206.03287.pdf">paper</a>]  [<a href="http://cs.yale.edu/homes/che/projects/nemf/">project page</a>] 

			</div>
		  </div>

		  <div class="row">
			<div class="column left" style="background-color:FFFF;">
				<iframe   src="https://www.youtube.com/embed/S09b57tO2N8" frameborder="0"  allowfullscreen></iframe>
			</div>
			<div class="column right" style="background-color:ffff;">
				<p>
					<b>SAMP:Stochastic Scene-aware Motion Prediction</b>
					<br>Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, <b>Yi Zhou</b> and  Michael J Black.		
					<br><em>ICCV 2021 </em>[<a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/652/samp.pdf">paper</a>]
					[<a href="https://samp.is.tue.mpg.de">project page</a>]</p>
			</div>
		  </div>

		  <div class="row">
			<div class="column left" style="background-color:FFFF;">
				<video width="80%" height="80%" autoplay="autoplay" loop muted source src="zhouyi_files/deepEmulator.mp4" type="video/mp4" controls></video>
			</div>
			<div class="column right" style="background-color:ffff;">
				<p><b>A Deep Emulator for Secondary Motion of 3D Characters</b>
					<br>Mianlun Zheng (Primary Intern), <b>Yi Zhou</b>, Duygu Ceylan, Jernej Barbic		
					<br><em>CVPR 2021 (Oral Presentation) </em>[<a href="https://arxiv.org/abs/2103.01261">paper</a>]
					[<a href="http://barbic.usc.edu/deepEmulator/index.html">project page</a>][<a href="https://github.com/ZhengMianlun/deep_emulator">code</a>]</p>
			
			</div>
		  </div>

		  <div class="row">
			<div class="column left" style="background-color:FFFF;">
				<video width="80%" height="80%" source src="https://generativetweeningprojectpage.s3.amazonaws.com/input_whole.mp4" type="video/mp4" controls></video>
			</div>
			<div class="column right" style="background-color:ffff;">
				<p><b>Generative Tweening: Long-term Inbetweening of 3D Human Motions</b>
					<br><b>Yi Zhou</b>, Cynthia Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, Hao Li
					<br>
					[<a href="https://arxiv.org/pdf/2005.08891.pdf">paper</a>]
					[<a href="project_tweening/GenerativeTweening.html">project page</a>]</p>
			</div>
		  </div>
		
		  <div class="row">
			<div class="column left" style="background-color:FFFF;">
				<iframe  src="https://www.youtube.com/embed/AWlpNeOzMig?rel=0&amp;controls=1&amp;showinfo=0;loop=1;autoplay=1" muted frameborder="0"  allowfullscreen></iframe>
					</div>
			<div class="column right" style="background-color:ffff;">
				<p><b>Auto-conditioned Recurrent Networks for Extended Complex Human Motion Synthesis</b>
					<br><b>Yi Zhou*</b>, Zimo Li*, Shuangjio Xiao, Chong He, Zeng Huang, Hao Li
					<br><em>ICLR 2018</em>
					[<a href="https://arxiv.org/pdf/1707.05363.pdf">paper</a>]
					[<a href="https://youtu.be/zI1HOyruYcY">video</a>]</p> 
			</div>
		  </div>
	</div>

	<hr class="myhr"></hr>


	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Human Capturing</h1>
		<p>This part of work is about inferring 3D human face, hair, garment and pose from images and videos. This work can enable the end applications on novel human synthesis, e.g., swapping the face between two videos.
		<br>
		<br>
		<b>Publications:</b> 
			
			<div class="row">

				<div class="row">
					<div class="column left" style="background-color:FFFF;">
						<!-- <img src="https://huggingface.co/wlyu/FaceLift/resolve/main/images/teaser.png" width="40%" /> -->
						<video autoplay muted playsinline loop src="https://huggingface.co/wlyu/FaceLift/resolve/main/videos/teaser.mp4" type="video/mp4"  width="80%"></video>
						  <!-- <video muted="" loop="" autoplay="" width = "100%" style="text-align: center">
							 
							<source src="https://huggingface.co/wlyu/FaceLift/resolve/main/videos/teaser.mp4"  type="video/mp4">
						  </video>

						  <img src="https://huggingface.co/wlyu/FaceLift/resolve/main/images/teaser.png" style="text-align: bottom" width="30%"> -->
						  
					</div>
					
					<div class="column right" style="background-color:ffff;">
						<p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> 
						<p><b>FaceLift: Single Image to 3D Head with View Generation and GS-LRM</b>
							<br>Weijie Lyu, <b>Yi Zhou</b>, Ming-Hsuan Yang, Zhixin Shu
							<br><em>2024</em> [<a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.wlyu.me/FaceLift/static/pdf/FaceLift_arxiv_version.pdf">paper</a>] [<a href="https://www.wlyu.me/FaceLift/">project</a>][<a href="https://github.com/weijielyu/FaceLift">code</a>]		
							
					</div>
				  </div>

				  <p>&nbsp;</p> 
					<p>&nbsp;</p> 
					<p>&nbsp;</p> 
					<p>&nbsp;</p> 
					<p>&nbsp;</p> 


				<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./zhouyi_files/2023_yasamin.png" width="90%" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Normal-guided Garment UV Prediction for Human Re-texturing</b>
						<br>Yasamin Jafarian, Tuanfeng Y. Wang, Duygu Ceylan, Jimei Yang, Nathan Carr, <b>Yi Zhou</b>, Hyun Soo Park
						<br><em>CVPR 2023 (Highlight Paper)</em> [<a href="https://arxiv.org/pdf/2303.06504.pdf">paper</a>] [<a href="https://www.yasamin.page/normal-guided-uv">project</a>]		
						<p>&nbsp;</p> 
				</div>
			</div>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./zhouyi_files/refu.jpg" width="90%" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>ReFU: A Repulsive Force Unit for Garment Collision Handling in Neural Networks</b>
						<br>Qingyang Tan (Primary Intern), <b>Yi Zhou</b>, Tuanfeng Wang, Duygu Ceylan, Xin Sun, Dinesh Manocha
						<br><em>ECCV 2022</em> [<a href="https://qytan.com/files/refu.pdf">paper</a>] [<a href="https://qytan.com/publication/refu/">project</a>][<a href="https://github.com/aldehydecho/ReFU">code</a>]		
						<p>&nbsp;</p> 
				</div>
			</div>

			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./zhouyi_files/visibility.jpg" width="90%" />
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Learning Visibility for Robust Dense Human Body Estimation</b>
						<br>Chun-Han Yao, Jimei Yang, Duygu Ceylay, <b>Yi Zhou</b>, Yang Zhou, Ming-Hsuan Yang
						<br><em>ECCV 2022</em> [<a href="https://arxiv.org/pdf/2208.10652.pdf">paper</a>][<a href="https://github.com/chhankyao/visdb">code</a>]		
						<p>&nbsp;</p> 
					</div>
				</div>


		  
			<div class="row">
				<div class="column left" style="background-color:FFFF;">
					<img src="./zhouyi_files/123faceICCV.png" width="90%"/>
				</div>
				<div class="column right" style="background-color:ffff;">
					<p><b>Realistic Dynamic Facial Textures from A Single Image Using GANs</b>
						<br>Kyle Olszewski, Zimo Li, Chao Yang, <b>Yi Zhou</b>, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, Hao Li
						<br><em>ICCV 2017</em> [<a href="http://www.hao-li.com/publications/papers/iccv2017RDFTFSIUG.pdf">paper</a>] [<a href="https://youtu.be/R-kkq4xxVNw">video</a>] [<a href="http://www.hao-li.com/publications/additionalMaterials/iccv2017additionalMaterialsB.pdf">additional materials</a>]
						<p>&nbsp;</p> 
				</div>
			</div>
		</div>

	<div><br></div>

	<hr class="myhr"></hr>


	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Augmented Reality</h1>
		<p>Inspired by the hologram system in manga "Psycho Pass" I design the Pmomo (projection mapping on movable object) system to create the phantasm of real-world objects being covered with virtual exteriors. As supporting 6-DOF object motion, the system can keep tracking the object and projecting 3D texture on its surface in real-time. Meanwhile, occlusions are culled from projection as to improve the sense of realism. In the picture left, the models held in the user' hands were originally white but now rendered with vivid textures by projection. </p>
		
		<p>For watching the demonstration videos: [<a href="https://youtu.be/K3KLlAA4pqY" target="_blank">preview</a>] [<a href="https://youtu.be/duCLF4rbYDc" target="_blank">surprise</a>] [<a href="https://youtu.be/4W3z9JyKmt4">demo</a>]</p>
		<br>
		<b>Publication:</b>
		
		<p>PMOMO: PROJECTION MAPPING ON MOVABLE 3D OBJECT
		<br><b>Yi Zhou</b>, Shuangjiu Xiao, Ning Tang, Zhiyong Wei, Xu Chen
		<br><em>CHI 2016</em> [<a href="https://dl.acm.org/citation.cfm?id=2858329">paper</a>]</p>
		<br>
		<img src="./zhouyi_files/pmomo.jpg" />
			
		
	</div>
	
	
	<hr class="myhr"></hr>
	

	<div class="project-contatiner w-container">
		
		<h1 class="project-heading" >Innovation Projects</h1>

		<div class="w-row project-row"><div class="w-col w-col-5"><img src="./zhouyi_files/poseable.gif"></div>
		<div class="w-col w-col-7">
		<p><b>Project Poseable</b></p> represents a breakthrough in AI-based image generation models, making it easy for the image generator to seamlessly interact with large 3D objects, including poses from photos of real people. The prototype is integrated with Project Poseable which can create a 3D rendering from text input, take depth and perspective into consideration, and re-align the object. While creating 3D scenes with traditional skeletal systems is technically complex and time-consuming, Project Poseable offers an effortless alternative that makes it possible for anyone to expand their creative horizons. Watch the <a href="https://youtu.be/y9G3dAeoKDQ?si=6sIb7iocqb5AljJR">live demo</a> at Adobe Max Sneaks 2023.
		</p></div>
		</div></div>


		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><div data-animation="slide" data-duration="500" data-infinite="1" class="w-slider"><div class="w-slider-mask"><div class="w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/5667bbfb53f0878447273aa5_getandput1.jpg"></div><div class="slider-2 w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/getandput.png"></div><div class="slider-2 w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/getandput4.png"></div><div class="w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/5667bc1056cef67b477bf6a1_getandput2.jpg"></div></div><div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div><div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div><div class="w-slider-nav w-round"><div class="w-slider-dot w-active" data-wf-ignore=""></div><div class="w-slider-dot" data-wf-ignore=""></div></div></div></div>
		<div class="w-col w-col-7">
		<p><b>Get&amp;Put</h1></b></p>It is a cloud powered magic that enable users to grab pictures and music directly from the screen, carrying around the data and put them 'into' other smart devices. Through hand commands, the files are transferred from device-to-device without any extra equipment. For watching the videos:
		[<a href="http://youtu.be/y-kg4rBbSgU">fun</a>] [<a href="http://youtu.be/gpjOyr89q5Y">child</a>] [<a href="http://youtu.be/bm1NE5nQNyc">demo</a>]</p>
		<div class="highlight-text">It won the Global 3rd place of 2013 Imagine Cup - Azure Challenge and the 3rd place of 2013 Imagine Cup - Mail. Ru Awards</div></p></div>
		</div></div>



		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img src="./zhouyi_files/5667b74dedabe1ea47f18f28_clio1.jpg"></div>
		<div class="w-col w-col-7">
		<p><b>Clio Super Painter</b></p>
		<p>Clio is an android app that makes painting a new way of communication. It allows users to do synchronous painting and picture transmission on more than three Android devices via Wi-Fi and Bluetooth.&nbsp;To make it more fun, Clio also generates special effects when 'telepathy' happens between the users.<br>For downloading the apps and watching the videos: 
		[<a href="https://www.youtube.com/watch?v=Fbb_CXxDECU">short</a>] [<a href="https://www.youtube.com/watch?v=Zxa9EKbRirc">fun</a>]</p>
		<div class="highlight-text">It won the <b>Global 1st place of 2012 Ericsson Application Awards</div></div></div>

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img width="375" src="./zhouyi_files/5546169eabdddf0146915c14_avat.png"></div><div class="w-col w-col-7">
				<p><b>AVAT: Automatic Visual Activity Test</b></p><p>AVAT is a medical application based on Kinect, and it can offer multiple self-catering vision test items, including: visual acuity test, chromatoptometry test and stereopsis vision test. It is very easy and convenient for hospital and home usage. Plus, AVAT can track the vision test results, helping the specialists to assess the change of visual condition, e.g. parents willing to monitor the vision acuity of their children. With the popularization of the AVAT system in medical institutions, schools and communities, it could solve the issue&nbsp; of the overloaded physical examinations in China.</p><div class="highlight-text">It won the Chinese local 2rd prize of 2012 Imagine Cup&nbsp;</div></div></div></div>
					

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img src="./zhouyi_files/5667ba7053f0878447273a7f_motion1.jpg"></div><div class="w-col w-col-7">
			<p><b>Human Motion Recognition Based on Joint Motion Image</b></p><p>In the field of human motion recognition methods, the objective of this research is to transform motions into images and accurately recognize the motion images, under very small training sets.</p></div></div></div>

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img width="361" src="./zhouyi_files/554614efabdddf0146915c01_spongebob.png"></div><div class="w-col w-col-7">
			<p><b>Performance-based 3D Cartoon Facial Animation Control</b></p><p>I developed this software to create a windows application that can make the avatars follow the facial and body actions of the users in real time. In the left picture, I am controlling 'Sponge Bob' to perform funny actions of nodding and laughing. My method is building several key facial expression models, and morphing them into a new expression according to the facial coefficients tracked by Kinect.</p></div></div></div>

		
		
	</div>
	
	<div><br></div>

</div>

<div id="music" class="w-section section press" >
	<div class="project-contatiner w-container"><h1 class="main-heading">Music</h1>

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5">
			<iframe width="100%" height="100%" src="https://www.youtube.com/embed/-h6xl0Yf44g" frameborder="0"  allowfullscreen></iframe></div>
		<div class="w-col w-col-7">
		<p>痛苦都来自幻想 written, arranged and performed by Julian Hou and Yi Zhou.</p>
		</div></div>

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5">
			<iframe width="100%" height="100%" src="https://www.youtube.com/embed/uvhyHRwfSQo" frameborder="0"  allowfullscreen></iframe></div>
		<div class="w-col w-col-7">
		<p>Cover song of 春よ、来い (Haru yo, koi) rearranged and performed by Julian Hou and Yi Zhou for celebrating the arrival of spring.</p>
		</div></div>

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5">
			<iframe width="100%" height="100%" src="https://www.youtube.com/embed/sYpf7kgWpNU" frameborder="0"  allowfullscreen></iframe></div>
		<div class="w-col w-col-7">
		<p>Il mio bel foco (B. Marcello) performed by Yi Zhou and Chenxi Liu.</p>
		</div></div>

		<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5">
			<iframe width="100%" height="100%" src="https://www.youtube.com/embed/eCC4WVt1MRU" frameborder="0"  allowfullscreen></iframe></div>
		<div class="w-col w-col-7">
		<p>"Concerto pour deux voix" rearranged and performed by Yi Zhou and Julian Hou.</p>
		</div></div>


		
		

</div>

</div>
</div>

<div class="w-section section footer copyright"><div class="w-container"><div>COPYRIGHT 2015. ALL RIGHTS RESERVED</div></div></div>
<div class="w-col w-col-2">
	<div style="width:100px"><script type="text/javascript" id="clstr_globe" src="https://cdn.clustrmaps.com/globe.js?d=6Hgf3YMgOYj6NWfc29ulxldS2RV6PES-R2sn7qRa9Jc"></script>
</div>
<script type="text/javascript" src="./zhouyi_files/jquery.min.js"></script>
<script type="text/javascript" src="./zhouyi_files/webflow.72472ab46.js"></script>
<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
