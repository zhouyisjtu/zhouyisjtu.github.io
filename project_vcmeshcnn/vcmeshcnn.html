
<!DOCTYPE html>
<html data-wf-domain="yis-first-project-687ef6.webflow.io" data-wf-page="596e65d120426e09785027f0" data-wf-site="596e65d120426e09785027eb" data-wf-status="1">
    <head>
        <meta charset="utf-8"/>
        <title>GenerativeTweening</title>
        <meta content="width=device-width, initial-scale=1" name="viewport"/>
        <meta content="Webflow" name="generator"/>
        <link href="./project_vcmeshcnn.css" rel="stylesheet" type="text/css"/>
        <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
        <script type="text/javascript">WebFont.load({  google: {    families: ["Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic","Roboto:300,regular,500"]  }});</script><!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->
        <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
        <link href="https://uploads-ssl.webflow.com/img/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
        <link href="https://uploads-ssl.webflow.com/img/webclip.png" rel="apple-touch-icon"/>
    </head>
    
    <body>
        <div class="section">
            <h1 class="section-heading centered">Fully Convolutional Mesh Autoencoder</h1>
            <div class="section-subheading center"> using Efficient Spatially Varying Kernels</div>
            <div class="w-container">
                <div class="text-block"><br/></div>
                <a href="https://arxiv.org/pdf/2006.04325.pdf">paper</a>&nbsp; &nbsp; &nbsp;<a href="https://github.com/facebookresearch/VCMeshConv">code</a>
                <p class="paragraph">In this project, we propose a fully convolutional mesh autoencoder for arbitrary registered mesh data. It is enabled by our novel convolution and (un)pooling operators learned with globally shared weights and locally varying coefficients which can efficiently capture the spatially varying contents presented by irregular mesh connections. Our model outperforms state-of-the-art methods on reconstruction accuracy. In addition, the latent codes of our network are fully localized thanks to the fully convolutional structure, and thus have much higher interpolation capability than many traditional 3D mesh generation models.</p>
            </div>
            <div class="w-container">
                <h4 class="heading-3">Network Operators</h4>
                <p class="paragraph">We provide a complete set of convolutional network operators on mesh or graph that are analog to regular convolutional networks.</p>
                <img src="./networklayers.png" width="80%" height="80%" >
                <p class="paragraph"> </p>

                <h4 class="heading-3">Main Idea</h4>
                <p class="paragraph">The intuition is that we can imagine a discrete convolution kernel definedwith weights on a standard grid and we call them Weight Basis. The real vertices in a local regionof the mesh scatter in the grid.  The Weight Basis can be shared through the whole mesh, whilethe weights on those real vertices need to be sampled from the Weight Basis by different functionsfrom vertex to vertex. Another perspective for this intuition is that since a mesh is a discretizationof a continuous space and a continuous convolution kernel can be shared spatially on the originalcontinuous space, we should be able to resample the unique continuous kernel to generate the weightsfor each neighborhood of the vertex. To achieve that, the sampling functions need to be defined pervertex locally. Rather than using a handcrafted sampling functions , we learn them through training. Specifically, we compute the weights per each vertex,as the linear combination of the Weight Basis with <b>locally variant coefficients (vc)</b>. And we call our convolution and transpose layers as vcConv and vcTransConv</p>
                <img src="./weightbasis.png" width="30%" height="30%" >
                <p class="paragraph">As for pooling and unpooling, we assume the vertex densities are un-even across the mesh, so apply Monte Carlo sampling for feature aggregation. While itâ€™s hard to design agenerally rational density estimation function, we let the network learn the optimal <b>variant density (vd)</b> coefficients across all the training samples. Thus, we named our pool and unpool layers as vdPool/vdUnpool.</p>
                

                <h4 class="heading-3">Applications</h4>
                <p class="paragraph">1. D-FUAST Human Body Mesh</p>
                <p class="paragraph">In this example, we trained an autoencoder with 32933 registered human body meshes from <a href="http://dfaust.is.tue.mpg.de/">D-FAUST</a> dataset. The original mesh contains the 3D positions of 6890 vertices and our network compresses it with four down-sampling residual blocks to a small graph with 7 vertices, 9 dimensional latent vector on each vertex, and reconstructs it back with four up-sampling residual blocks to the original mesh.</p>
                
                <img src="./dfaust_network.png" width="100%" height="100%" >

                <p class="paragraph">The right most figure is the visualization of the receptive field of the middle layer. One can see that the vertices distribute at the end of the limbs and the head,  their receptive fields propagate out and their influence smoothly fades out. This means that each vertex's latent code will infect more on it's surrounding region rather than the whole mesh, which will enable us to locally manipulate the outcome geometry through modifying the latent codes.</p>
                
                <p class="paragraph">Below visualizes the reconstruction error of our network and other types of graph convolutional networks.</p>
                
                <img src="./dfaust_comparison.png" width="100%" height="100%" >
                
                <p class="paragraph">Following we show the case of pose transfering through replacing the local latent code. We first run the encoder on both Man A and Man B to obtain their latent codes, then we replace the latent code of the vertex on Man A's leg with the latent code on Man B's leg and feed them to the decoder. The new Man A's left leg now changes to the pose of Man B's left leg, but interestingly still remain the original shape.</p>
                <img src="./dfaust_latentcode.png" width="90%" height="90%" >

                <p class="paragraph">&nbsp; </p>
                <p class="paragraph">2. Hand Mesh with Textures</p>
                <img src="./hand_latentvertices.png" width="80%" height="80%" >
                <p class="paragraph">In this example, we trained an autoencoder on a registered 3D hand dataset. The dataset was captured by Facebook Reality Labs internally. The hand mesh contains about 40k vertices with 3d coordinates and RGB color on each vertex. We set the latent vertices to be at the tips of the five fingers and the wrist. (yes, you can customize the choice of latent vertices using our code). The following video shows the reconstruction results on the test set.</p>
                <video width="70%" height="70%" source src="./hand_AE.mp4" type="video/mp4" controls autoplay loop></video>
                <p class="paragraph">Below we interpolate two hand meshes. We first inferred the latent codes from a source mesh and a target mesh, then we linearly interpolatedthe latent code on each individual latent vertex separately. </p>

                
                <div class="container">
                    <video id="hand_inter" width="70%" height="70%" source src="./hand_interpolation.mp4" type="video/mp4" controls autoplay loop></video>
                </div>

                <p class="paragraph">3. High Resolution Human Body Mesh</p>
                <img src="./highres_body.png" width="100%" height="100%" >
                <p class="paragraph">3D data in real applications can have very high resolution. Therefore, we experiment our network ona high-resolution human dataset that contains 24,628 fully aligned meshes, each with 154k vertices and 308k triangles. The bottleneck contains 18 vertices and 64 dimensions per vertex, resulting in a compression rate of 0.25%. Below are part of the results on the test set. </p>
                <div class="container">
                    <video id="highres_body" width="70%" height="70%" source src="./highres_body.mp4" type="video/mp4" controls autoplay loop></video>
                </div>

                <p class="paragraph">4. Tetrahedrons Mesh</p>
                <img src="./tetmesh.png" width="100%" height="100%" >
                <p class="paragraph"> Tet meshes are commonly used for physical simulation. In this case, we simulated 10k frames of deformed tet meshes of an Asian dragon and trained an autoencoder on those tet meshes. </p>
                <video width="70%" height="70%" source src="./tetmesh_dragon_AE.mp4" type="video/mp4" controls autoplay loop></video>

                <p class="paragraph">4. Non-manifold Mesh</p>
                
                <p class="paragraph"> Unlike many other mesh convolutional networks, ours don't have limitation to manifold meshes. Below is an example of training an autoencoder on a tree model which contains multiple components and many non-manifold structures. </p>
                <video width="70%" height="70%" source src="./nonmanifold_tree_AE.mp4" type="video/mp4" controls autoplay loop></video>
                






        <div class="section"></div>
        <div class="footer center">
            <div class="w-container">
                
                <div class="footer-text">Contact: yizho@adobe.com</div>
            </div>
        </div>
        <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js?site=596e65d120426e09785027eb" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
        <script src="https://uploads-ssl.webflow.com/596e65d120426e09785027eb/js/webflow.3cd0ca831.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
    </body>
</html>

